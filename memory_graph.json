{
  "file_chunker_python_function_PythonCodeChunker_visit_AsyncFunctionDef_5": {
    "id": "file_chunker_python_function_PythonCodeChunker_visit_AsyncFunctionDef_5",
    "raw": "    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):\n        \"\"\"Handles async function definitions.\"\"\"\n        identifier = f\"{self.current_class_name}.{node.name}\" if self.current_class_name else node.name\n        chunk_content = self._get_node_content(node)\n        parent = self.current_class_name or self.file_identifier\n        self._add_chunk(chunk_content, \"python_function_async\", identifier, node, parent)\n",
    "summary": "The function processes and records information about asynchronous functions defined in Python code.",
    "key_concepts": [
      "Async function definition handling",
      "Identifier generation for async functions",
      "Chunk content extraction from AST nodes",
      "Parent class or file identification",
      "Adding chunk to storage with metadata"
    ],
    "tags": [
      "ast",
      "async function definition",
      "python code",
      "ast.NodeVisitor",
      "_visit_AsyncFunctionDef",
      "__main__",
      "class name",
      "file identifier",
      "chunk content",
      "add chunk",
      "current_class_name",
      "self.file_identifier",
      "self.current_class_name",
      "NodeVisitor subclass",
      "node attribute",
      "parent variable",
      "get_node_content method",
      "Python AST manipulation"
    ],
    "sequence_index": 4,
    "parent_identifier": "PythonCodeChunker",
    "dependencies": [
      "current_class_name",
      "file_identifier",
      "_current_class_name",
      "_file_identifier",
      "self.xyz",
      "import ast",
      "import _get_node_content from somewhere",
      "import add_chunk from somewhere"
    ],
    "produced_outputs": [
      "identifier",
      "parent"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_chunk_markdown_8": {
    "id": "file_chunker_python_function_chunk_markdown_8",
    "raw": "def chunk_markdown(text: str, file_identifier: str = \"unknown_markdown\") -> List[Dict[str, Any]]:\n    \"\"\"Chunks Markdown text by level 2 headers (##).\"\"\"\n    chunks = []\n    parts = re.split(r'(\\n##\\s+.*)', text) # Split before headers\n    current_content = \"\"\n    current_header = \"Introduction\" # Default header\n    sequence_counter = 0 # Sequence index for markdown sections\n\n    if parts and parts[0].strip():\n        current_content = parts[0].strip()\n\n    for i in range(1, len(parts), 2):\n        header_line = parts[i].strip()\n        content_after_header = parts[i+1] if (i + 1) < len(parts) else \"\"\n        header_identifier = re.sub(r'^##\\s*', '', header_line)\n\n        if current_content: # Save previous section\n            chunks.append({\n                \"content\": current_content,\n                \"metadata\": {\n                    \"type\": \"markdown_section\",\n                    \"identifier\": current_header,\n                    \"parent_identifier\": file_identifier, # File is parent of sections\n                    \"sequence_index\": sequence_counter\n                }\n            })\n            sequence_counter += 1\n\n        current_header = header_identifier\n        current_content = content_after_header.strip() # Content for *this* header\n\n    if current_content: # Add the last section\n        chunks.append({\n            \"content\": current_content,\n            \"metadata\": {\n                \"type\": \"markdown_section\",\n                \"identifier\": current_header,\n                \"parent_identifier\": file_identifier,\n                \"sequence_index\": sequence_counter\n            }\n        })\n        sequence_counter += 1\n\n    # Handle case where there are no '##' headers\n    if not chunks and text.strip():\n         chunks.append({\n             \"content\": text.strip(),\n             \"metadata\": {\"type\": \"markdown_file\", \"identifier\": file_identifier, \"parent_identifier\": None, \"sequence_index\": 0}\n         })\n\n    logging.info(f\"Chunked Markdown '{file_identifier}' into {len(chunks)} sections.\")\n    return chunks\n",
    "summary": "The function `chunk_markdown` divides a given markdown text by level 2 headers (##) and organizes the content under each header.",
    "key_concepts": [
      "Chunking",
      "Level 2 headers (`##`)",
      "Split text by `\\n## ...`",
      "Default header: \"Introduction",
      "Sequence index for markdown sections",
      "Save previous section content before new header starts",
      "Append metadata with type, identifier, parent_identifier, sequence_index",
      "Handle case without level 2 headers (whole file as one chunk)",
      "Logging information about the number of chunks created"
    ],
    "tags": [
      "text processing",
      "markdown parsing",
      "regular expressions",
      "python function",
      "data structures",
      "sequence indexing",
      "file metadata",
      "chunking algorithm",
      "text splitting",
      "header identification"
    ],
    "sequence_index": 7,
    "parent_identifier": "chunker",
    "dependencies": [
      "`re`",
      "`logging`"
    ],
    "produced_outputs": [
      "current_content",
      "chunks",
      "header_identifier",
      "sequence_counter"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_PythonCodeChunker__get_node_content_2": {
    "id": "file_chunker_python_function_PythonCodeChunker__get_node_content_2",
    "raw": "    def _get_node_content(self, node: ast.AST) -> str:\n        if not hasattr(node,'lineno')or not hasattr(node,'end_lineno'):\n            return\"\"\n\n        start_line=node.lineno-1\n        end_line=node.end_lineno\n\n        if hasattr(node,'decorator_list')and node.decorator_list:\n            try:\n                min_decorator_line=min(d.lineno for d in node.decorator_list if hasattr(d,'lineno'))\n                start_line=min(start_line,min_decorator_line-1)\n            except ValueError:\n                pass\n\n        start_line=max(0,start_line)\n        end_line=min(end_line,len(self.source_lines))\n        return\"\".join(self.source_lines[start_line:end_line])\n",
    "summary": "The function extracts the source code content of an AST node, adjusting for decorators and line numbers.",
    "key_concepts": [
      "Main topics: AST manipulation",
      "Key arguments/claims:",
      "Handling node attributes in Python's Abstract Syntax Tree (AST)",
      "Line number adjustments for decorators",
      "Important steps or core terminology used:",
      "`ast.AST`",
      "Node attributes (`lineno`, `end_lineno`)",
      "Source lines handling",
      "Decorator line adjustment"
    ],
    "tags": [
      "ast parsing",
      "python source code",
      "ast.AST",
      "lineno attribute",
      "decorator_list",
      "error handling",
      "string manipulation",
      "source lines extraction"
    ],
    "sequence_index": 1,
    "parent_identifier": "PythonCodeChunker",
    "dependencies": [
      "source_lines",
      "ast",
      "hasattr",
      "min",
      "max"
    ],
    "produced_outputs": [
      "start_line",
      "end_line",
      "source_lines"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_PythonCodeChunker_visit_FunctionDef_4": {
    "id": "file_chunker_python_function_PythonCodeChunker_visit_FunctionDef_4",
    "raw": "    def visit_FunctionDef(self, node: ast.FunctionDef):\n        \"\"\"Handles function definitions.\"\"\"\n        identifier = f\"{self.current_class_name}.{node.name}\" if self.current_class_name else node.name\n        chunk_content = self._get_node_content(node)\n        # Parent is class name if inside a class, otherwise the file identifier\n        parent = self.current_class_name or self.file_identifier\n        self._add_chunk(chunk_content, \"python_function\", identifier, node, parent)\n",
    "summary": "The code defines how to process and record Python function definitions.",
    "key_concepts": [
      "Function definition handling",
      "Identifier generation for functions",
      "Chunk content extraction from AST nodes",
      "Parent determination based on context (class or file)",
      "Adding chunk to a collection with metadata"
    ],
    "tags": [
      "ast.function_def",
      "python_code",
      "function_handling",
      "ast.NodeVisitor",
      "code_snippet",
      "class_structure",
      "file_identification",
      "chunk_generation",
      "variable_assignment"
    ],
    "sequence_index": 3,
    "parent_identifier": "PythonCodeChunker",
    "dependencies": [
      "current_class_name",
      "file_identifier",
      "self.xyz",
      "import ast",
      "def _get_node_content(self, node: ast.FunctionDef): ...",
      "def _add_chunk(self, chunk_content: str, language: str, identifier: str, node: ast.FunctionDef, parent: str) ..."
    ],
    "produced_outputs": [
      "parent",
      "identifier",
      "chunk_content",
      "file_identifier"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_PythonCodeChunker__add_chunk_3": {
    "id": "file_chunker_python_function_PythonCodeChunker__add_chunk_3",
    "raw": "    def _add_chunk(self, content: str, node_type: str, identifier: str, node: ast.AST, parent_id: Optional[str]):\n        \"\"\"Helper to create and add a chunk dictionary.\"\"\"\n        if content:\n            self.chunks.append({\n                \"content\": content,\n                \"metadata\": {\n                    \"type\": node_type,\n                    \"identifier\": identifier,\n                    \"parent_identifier\": parent_id, # Added parent\n                    \"sequence_index\": self.sequence_counter, # Added sequence\n                    \"start_line\": getattr(node, 'lineno', None), # Use getattr for safety\n                    \"end_line\": getattr(node, 'end_lineno', None)\n                }\n            })\n            self.sequence_counter += 1 # Increment sequence number\n",
    "summary": "The function `_add_chunk` appends a new dictionary containing content and metadata to the `chunks` list.",
    "key_concepts": [
      "_add_chunk function definition",
      "content parameter check",
      "appending to chunks list",
      "metadata dictionary creation",
      "node_type inclusion in metadata",
      "identifier assignment for chunk and parent",
      "start_line attribute retrieval from AST node",
      "end_line attribute retrieval using getattr with default None",
      "sequence_counter increment"
    ],
    "tags": [
      "ast parsing",
      "python code",
      "ast.AST",
      "metadata handling",
      "sequence counter",
      "lineno",
      "end_lineno",
      "parent identifier",
      "chunk dictionary",
      "sequence incrementing"
    ],
    "sequence_index": 2,
    "parent_identifier": "PythonCodeChunker",
    "dependencies": [
      "`self.chunks`",
      "`ast.AST`",
      "`Optional[str]`"
    ],
    "produced_outputs": [
      "self.chunks",
      "self.sequence_counter"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_PythonCodeChunker_visit_ClassDef_6": {
    "id": "file_chunker_python_function_PythonCodeChunker_visit_ClassDef_6",
    "raw": "    def visit_ClassDef(self, node: ast.ClassDef):\n        \"\"\"Handles class definitions and visits methods within them.\"\"\"\n        class_identifier = node.name\n        # Decide if we want a node for the class itself (Optional)\n        # If so, its parent would be the file identifier\n        # class_content = self._get_node_content(node) # Get full class content\n        # self._add_chunk(class_content, \"python_class\", class_identifier, node, self.file_identifier)\n\n        # Process methods *within* the class\n        original_class_name = self.current_class_name\n        self.current_class_name = class_identifier # Set parent context for methods\n        # Explicitly visit function defs within the class body\n        for item in node.body:\n            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                self.visit(item)\n        self.current_class_name = original_class_name # Restore context\n",
    "summary": "The `visit_ClassDef` method processes and visits methods defined inside a given Python class definition.",
    "key_concepts": [
      "Class definition handling",
      "Node traversal in AST",
      "Python class identification and processing",
      "Context management for nested structures",
      "Visiting function definitions within a class body",
      "Restoring previous state after visiting methods"
    ],
    "tags": [
      "python",
      "classes",
      "methods",
      "function definitions",
      "asynchronous functions",
      "decorators",
      "abstract syntax tree",
      "code traversal",
      "class definition handling"
    ],
    "sequence_index": 5,
    "parent_identifier": "PythonCodeChunker",
    "dependencies": [
      "self.xyz",
      "imports: None",
      "globals: None",
      "external functions/methods:",
      "_get_node_content",
      "add_chunk",
      "visit",
      "astClassDef"
    ],
    "produced_outputs": [
      "class_identifier",
      "original_class_name",
      "current_class_name"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_PythonCodeChunker___init___1": {
    "id": "file_chunker_python_function_PythonCodeChunker___init___1",
    "raw": "    def __init__(self, source_code: str, file_identifier: str = \"unknown_file\"):\n        self.source_lines = source_code.splitlines(keepends=True)\n        self.chunks: List[Dict[str, Any]] = []\n        self.current_class_name: Optional[str] = None\n        self.file_identifier = file_identifier # Store filename for top-level parent ID\n        self.sequence_counter = 0 # Counter for chunk order\n",
    "summary": "The code defines a class to parse and store structured information from source files.",
    "key_concepts": [
      "Initialization of class with parameters",
      "Source code splitting into lines",
      "List to store chunks as dictionaries",
      "Current class name tracking",
      "File identifier storage",
      "Sequence counter initialization"
    ],
    "tags": [
      "source_code parsing",
      "class detection",
      "code analysis",
      "string manipulation",
      "sequence tracking",
      "python programming",
      "data structures",
      "object-oriented programming",
      "identifier extraction",
      "file handling",
      "text processing",
      "variable assignment",
      "function definition",
      "error checking",
      "modular design",
      "coding standards",
      "software development",
      "language syntax",
      "automated testing",
      "debugging tools",
      "version control integration"
    ],
    "sequence_index": 0,
    "parent_identifier": "PythonCodeChunker",
    "dependencies": [
      "List",
      "Optional",
      "FileIdentifier",
      "SequenceCounter",
      "SourceCode",
      "Keepends",
      "Lines",
      "DictStrAny",
      "ClassName",
      "TopLevelParentID",
      "ChunkOrder"
    ],
    "produced_outputs": [
      "source_lines",
      "chunks",
      "current_class_name",
      "file_identifier",
      "sequence_counter"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_PythonCodeChunker_chunk_7": {
    "id": "file_chunker_python_function_PythonCodeChunker_chunk_7",
    "raw": "    def chunk(self) -> List[Dict[str, Any]]:\n        \"\"\"Parses the code and returns the collected chunks.\"\"\"\n        try:\n            tree = ast.parse(\"\".join(self.source_lines), type_comments=True)\n            # Visit only top-level functions and classes initially\n            for node in tree.body:\n                 if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n                     self.visit(node)\n            return self.chunks\n        except SyntaxError as e:\n             logging.error(f\"AST Parsing Error in {self.file_identifier}: {e}. Cannot chunk accurately.\", exc_info=True)\n             # Fallback: return whole file as one chunk with limited metadata\n             return [{\n                 \"content\": \"\".join(self.source_lines),\n                 \"metadata\": {\"type\": \"python_file_parse_error\", \"identifier\": self.file_identifier, \"sequence_index\": 0, \"parent_identifier\": None}\n             }]\n",
    "summary": "The `chunk` method parses Python code into top-level functions and classes while handling syntax errors by returning the entire file as a single chunk with limited metadata.",
    "key_concepts": [],
    "tags": [
      "ast parsing",
      "syntax error",
      "logging",
      "fallback mechanism",
      "python file parse error",
      "function definition",
      "class definition",
      "source lines",
      "exception handling",
      "metadata extraction"
    ],
    "sequence_index": 6,
    "parent_identifier": "PythonCodeChunker",
    "dependencies": [
      "file_identifier",
      "source_lines",
      "EXTRA REQUIRED FUNCTIONS/METHODS:",
      "visit",
      "logging.error",
      "ast.parse",
      "SyntaxError",
      "exc_info=True",
      "List[Dict[str, Any]]"
    ],
    "produced_outputs": [
      "self.chunks",
      "self.file_identifier",
      "source_lines"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_chunk_plain_text_9": {
    "id": "file_chunker_python_function_chunk_plain_text_9",
    "raw": "def chunk_plain_text(text: str, file_identifier: str = \"unknown_text\", max_chars: int = 1500) -> List[Dict[str, Any]]:\n    \"\"\"Chunks plain text by paragraphs, splitting large paragraphs.\"\"\"\n    chunks = []\n    paragraphs = re.split(r'\\n\\s*\\n', text)\n    sequence_counter = 0\n\n    for i, paragraph in enumerate(paragraphs):\n        paragraph = paragraph.strip()\n        if not paragraph: continue\n\n        if len(paragraph) <= max_chars:\n            chunks.append({\n                \"content\": paragraph,\n                \"metadata\": {\"type\": \"text_paragraph\", \"identifier\": f\"Paragraph {i+1}\", \"parent_identifier\": file_identifier, \"sequence_index\": sequence_counter}\n            })\n            sequence_counter += 1\n        else:\n            start, para_chunk_counter = 0, 1\n            while start < len(paragraph):\n                end = start + max_chars \n                split_pos = paragraph.rfind(' ', start, end)\n                if split_pos != -1 and end < len(paragraph): end = split_pos + 1\n                elif end >= len(paragraph): end = len(paragraph)\n                chunk_content = paragraph[start:end].strip()\n                if chunk_content:\n                    chunks.append({\"content\": chunk_content, \"metadata\": {\"type\": \"text_split\", \"identifier\": f\"Paragraph {i+1} Part {para_chunk_counter}\", \"parent_identifier\": file_identifier, \"sequence_index\": sequence_counter}})\n                    sequence_counter += 1\n                    para_chunk_counter += 1\n                start = end\n\n    logging.info(f\"Chunked Text '{file_identifier}' into {len(chunks)} paragraphs/splits.\")\n    return chunks\n",
    "summary": "The function `chunk_plain_text` divides large plain text documents by paragraph, ensuring no single chunk exceeds a specified character limit.",
    "key_concepts": [
      "Chunk plain text by paragraphs",
      "Split large paragraphs based on max_chars limit",
      "Use regular expressions for splitting lines",
      "Strip whitespace from each paragraph",
      "Check if a paragraph is empty and skip it",
      "Append non-empty chunks to the list with metadata",
      "Handle long paragraphs that exceed max_chars using word boundaries",
      "Log total number of processed text segments"
    ],
    "tags": [
      "text processing",
      "regex",
      "string manipulation",
      "chunking",
      "metadata",
      "python",
      "text splitting",
      "sequence counter",
      "logging"
    ],
    "sequence_index": 8,
    "parent_identifier": "chunker",
    "dependencies": [
      "`re`",
      "`logging`"
    ],
    "produced_outputs": [
      "chunks",
      "sequence_counter",
      "para_chunk_counter"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_chunker_python_function_chunk_file_10": {
    "id": "file_chunker_python_function_chunk_file_10",
    "raw": "def chunk_file(file_path: str, file_content: str) -> List[Dict[str, Any]]:\n    \"\"\"Detects file type and applies the appropriate chunking strategy.\"\"\"\n    filename = os.path.basename(file_path)\n    base_filename_noext, extension = os.path.splitext(filename)\n    extension = extension.lower()\n\n    logging.info(f\"Chunking file: {filename} (type: {extension})\")\n\n    # Pass filename identifier to chunkers\n    if extension == \".py\":\n        # Use base filename as the top-level parent identifier for functions\n        chunker = PythonCodeChunker(file_content, file_identifier=base_filename_noext)\n        return chunker.chunk()\n    elif extension == \".md\":\n        return chunk_markdown(file_content, file_identifier=base_filename_noext)\n    elif extension in [\".txt\", \".log\"] or not extension:\n         return chunk_plain_text(file_content, file_identifier=base_filename_noext)\n    else:\n        logging.warning(f\"Unknown file type '{extension}'. Using plain text chunking.\")\n        return chunk_plain_text(file_content, file_identifier=base_filename_noext)\n",
    "summary": "The function `chunk_file` detects a file's extension and applies an appropriate strategy to split its content into chunks.",
    "key_concepts": [
      "Detects file type",
      "Applies appropriate strategy based on extension",
      "Logging information about the process",
      "Handles different extensions (.py, .md, .txt/.log)",
      "Uses specific chunkers for each format:",
      "PythonCodeChunker for \".py",
      "Chunk_markdown for \".md",
      "Plain text handling for others or unknown types"
    ],
    "tags": [
      "file processing",
      "python code",
      "markdown parsing",
      "log analysis",
      "text chunking",
      "os.path",
      "exception handling",
      "logging",
      "data structures",
      "type detection"
    ],
    "sequence_index": 9,
    "parent_identifier": "chunker",
    "dependencies": [
      "os",
      "logging",
      "PythonCodeChunker",
      "chunk_markdown",
      "chunk_plain_text"
    ],
    "produced_outputs": [
      "file_identifier",
      "chunker",
      "filename",
      "extension"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_processor_python_function_ChunkProcessor___init___1": {
    "id": "file_processor_python_function_ChunkProcessor___init___1",
    "raw": "    def __init__(self, digestor: Digestor, memory_graph: MemoryGraph, status_callback: Callable[[str], None], completion_callback: Callable[[bool], None]):\n        if not digestor: raise ValueError(\"Digestor instance required.\")\n        if not memory_graph: raise ValueError(\"MemoryGraph instance required.\")\n        self.digestor = digestor\n        self.memory_graph = memory_graph\n        self.status_callback = status_callback\n        self.completion_callback = completion_callback\n",
    "summary": "The code defines a class constructor that initializes an object with mandatory instances of Digestor and MemoryGraph, along with optional callbacks for status updates and task completion.",
    "key_concepts": [
      "Digestor initialization",
      "MemoryGraph validation",
      "Instance assignment",
      "Status callback registration",
      "Completion callback setup",
      "ValueError handling for missing instances"
    ],
    "tags": [
      "digestor initialization",
      "exception handling",
      "callable functions",
      "object-oriented programming",
      "python classes",
      "error raising",
      "callback mechanisms"
    ],
    "sequence_index": 0,
    "parent_identifier": "ChunkProcessor",
    "dependencies": [
      "Digestor",
      "MemoryGraph",
      "Callable[[str], None]",
      "Callable[[bool], None]"
    ],
    "produced_outputs": [
      "digestor",
      "memory_graph",
      "status_callback",
      "completion_callback"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_processor_python_function_ChunkProcessor__save_graph_4": {
    "id": "file_processor_python_function_ChunkProcessor__save_graph_4",
    "raw": "    def _save_graph(self):\n         try:\n            self.memory_graph.save_to_json(MEMORY_GRAPH_PATH)\n         except Exception as e:\n            logging.error(f\"Save graph error: {e}\", exc_info=True)\n            self.status_callback(f\"Status: Error saving memory\")\n",
    "summary": "The function '_save_graph' attempts to save a memory graph in JSON format and logs an error if it fails.",
    "key_concepts": [
      "Memory Graph Saving",
      "Error Handling",
      "Logging Error Message",
      "JSON Serialization",
      "File Path Specification",
      "Status Update Callback",
      "Key Arguments:",
      "Importance of error handling in file operations.",
      "Usefulness of logging for debugging purposes.",
      "Core Terminology:",
      "Memory Graph",
      "JSON",
      "Status Callback",
      "Logging",
      "Exception Handling"
    ],
    "tags": [
      "graph serialization",
      "exception handling",
      "json file writing",
      "status callback",
      "logging",
      "save operation",
      "memory management",
      "python function",
      "error reporting"
    ],
    "sequence_index": 3,
    "parent_identifier": "ChunkProcessor",
    "dependencies": [
      "MEMORY_GRAPH",
      "MEMORY_GRAPH_PATH",
      "self.xyz",
      "logging",
      "exc_info",
      "status_callback"
    ],
    "produced_outputs": [
      "memory_graph",
      "status_callback"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_processor_python_function_ChunkProcessor__digest_single_chunk_task_2": {
    "id": "file_processor_python_function_ChunkProcessor__digest_single_chunk_task_2",
    "raw": "    def _digest_single_chunk_task(self, chunk_data: Dict[str, Any], original_filename: str, index: int) -> Optional[MemoryNode]:\n        \"\"\"Digests a single chunk and adds linking info to the resulting node.\"\"\"\n        node: Optional[MemoryNode] = None\n        chunk_metadata = chunk_data.get(\"metadata\", {})\n        node_id = \"\"\n        try:\n            chunk_content = chunk_data.get(\"content\", \"\")\n            chunk_id_part = chunk_metadata.get(\"identifier\", f\"chunk_{index+1}\")\n            sanitized_chunk_id = re.sub(r'[^\\w\\-]+', '_', chunk_id_part)[:50]\n            base_filename_noext, _ = os.path.splitext(original_filename)\n            node_id = f\"file_{base_filename_noext}_{chunk_metadata.get('type','unknown')}_{sanitized_chunk_id}_{index+1}\"\n\n            logging.info(f\"Attempting digest node ID: {node_id} (Thread: {threading.current_thread().name})\")\n            node = self.digestor.digest(\n                chunk_content,\n                node_id=node_id,\n                chunk_metadata=chunk_metadata, # Pass metadata obj\n                generate_questions=False\n             )\n\n            if node:\n                node.sequence_index = chunk_metadata.get(\"sequence_index\")\n                node.parent_identifier = chunk_metadata.get(\"parent_identifier\")\n                setattr(node, 'source_chunk_metadata', chunk_metadata)\n                logging.info(f\"Success digest node {node_id}\")\n                return node\n            else:\n                logging.warning(f\"Digest returned None chunk {index+1} (Node ID: {node_id})\")\n                return None\n        except Exception as e:\n             log_id = node_id if node_id else f\"chunk_{index+1}\"\n             logging.error(f\"Error digest {log_id}: {e}\", exc_info=True)\n             return None\n",
    "summary": "The function `_digest_single_chunk_task` processes a single data chunk, generates its unique identifier and metadata-based linkings for it in the form of `MemoryNode`, then attempts to create this node using provided content. If successful or if an error occurs during processing (including digest failure), appropriate logging is performed; otherwise, None is returned indicating unsuccessful completion.",
    "key_concepts": [
      "Digest single chunk",
      "Chunk data processing",
      "Metadata extraction and sanitization",
      "Node ID generation",
      "Logging information flow",
      "Exception handling",
      "MemoryNode creation",
      "Sequence index assignment",
      "Parent identifier linkage",
      "KEY ARGUMENTS/CLAIMS:",
      "Sanitized unique node identification based on metadata and filename.",
      "Thread-safe logging for debugging purposes.",
      "CORE TERMINOLOGY/USAGE:",
      "Dict[str, Any]: Generic dictionary with arbitrary key-value pairs.",
      "MemoryNode: Custom class representing a memory-based data structure (not defined in snippet).",
      "digestor.digest(): Method to create or retrieve node based on content and metadata."
    ],
    "tags": [
      "digesting",
      "memory_node",
      "chunk_data",
      "metadata",
      "sanitization",
      "os.path",
      "threading",
      "exception handling",
      "log management",
      "file processing"
    ],
    "sequence_index": 1,
    "parent_identifier": "ChunkProcessor",
    "dependencies": [
      "os",
      "logging",
      "threading",
      "MemoryNode",
      "re",
      "digestor",
      "setattr"
    ],
    "produced_outputs": [
      "node",
      "node_id",
      "sequence_index",
      "parent_identifier",
      "source_chunk_metadata"
    ],
    "follow_up_questions": [],
    "source": null
  },
  "file_processor_python_function_ChunkProcessor_process_chunks_3": {
    "id": "file_processor_python_function_ChunkProcessor_process_chunks_3",
    "raw": "    def process_chunks(self, chunks: List[Dict[str, Any]], original_filename: str):\n         total_chunks = len(chunks)\n         nodes_to_add: List[MemoryNode] = []\n         max_workers = min(8, os.cpu_count() + 4) if os.cpu_count() else 8\n         if total_chunks == 0: \n            logging.info(\"No chunks.\")\n            self.status_callback(\"Status: No chunks.\")\n            self.completion_callback(False)\n            return\n\n         logging.info(f\"Processor start parallel digest ({total_chunks} chunks, max {max_workers} workers).\")\n         self.status_callback(f\"Status: Starting digestion ({total_chunks} chunks)...\")\n         with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix='DigestWorker') as executor:\n             future_to_chunk_index = { executor.submit(self._digest_single_chunk_task, chunk, original_filename, i): i for i, chunk in enumerate(chunks) }\n             processed_chunks = 0\n             for future in concurrent.futures.as_completed(future_to_chunk_index):\n                 chunk_index = future_to_chunk_index[future]\n                 try:\n                    node = future.result()\n                    (nodes_to_add.append(node) if node else None)\n                 except Exception as exc: logging.error(f'Chunk task {chunk_index + 1} generated exception: {exc}', exc_info=True)\n                 processed_chunks += 1\n                 if processed_chunks % 5 == 0 or processed_chunks == total_chunks: self.status_callback(f\"Status: Digested {processed_chunks}/{total_chunks} chunks...\")\n\n         logging.info(f\"Processor finished. {len(nodes_to_add)}/{total_chunks} chunks yielded nodes.\")\n         graph_changed = False\n         if nodes_to_add:\n             logging.info(f\"Adding {len(nodes_to_add)} nodes to MemoryGraph.\")\n             graph_changed = True\n             for node in nodes_to_add: self.memory_graph.add_node(node)\n             self._save_graph()\n         final_status = f\"Status: Finished {original_filename}. Added {len(nodes_to_add)}/{total_chunks} nodes.\"\n         final_status += \" (Check logs)\" if len(nodes_to_add) < total_chunks else \"\"\n         self.status_callback(final_status); self.completion_callback(graph_changed)\n",
    "summary": "The function processes a list of data chunks in parallel, computes their digest values using multiple workers based on CPU count or default to 8, and updates the memory graph with new nodes derived from these processed chunks.",
    "key_concepts": [
      "process_chunks function",
      "List[Dict[str, Any]]",
      "original_filename parameter",
      "Total chunks calculation",
      "Nodes to add list initialization",
      "Max workers determination based on CPU count or default value of 8",
      "Logging and status callbacks for no-chunk scenario",
      "Parallel processing with ThreadPoolExecutor using max_workers limit",
      "Digest single chunk task submission via concurrent.futures",
      "Handling exceptions during future results retrieval",
      "Progress logging every processed_chunk incremented by 5, total_chunks completion check",
      "Finalization: log success/failure of node addition to MemoryGraph and save graph state",
      "Completion callback with final status update"
    ],
    "tags": [
      "concurrent.futures",
      "threading",
      "multiprocessing",
      "logging",
      "exception handling",
      "memory graph",
      "digesting",
      "chunk processing",
      "os.cpu_count",
      "thread pool executor",
      "python code",
      "parallel computing",
      "data structures",
      "task scheduling",
      "error reporting",
      "concurrent programming",
      "asynchronous execution",
      "node addition",
      "file I/O operations",
      "process management",
      "dynamic worker allocation"
    ],
    "sequence_index": 2,
    "parent_identifier": "ChunkProcessor",
    "dependencies": [
      "os",
      "concurrent.futures",
      "logging",
      "MemoryNode",
      "self._digest_single_chunk_task",
      "self.memory_graph.add_node",
      "self.status_callback",
      "self.completion_callback",
      "self._save_graph"
    ],
    "produced_outputs": [
      "nodes_to_add",
      "graph_changed",
      "final_status"
    ],
    "follow_up_questions": [],
    "source": null
  }
}